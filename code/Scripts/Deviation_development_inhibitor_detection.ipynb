{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d38a669",
   "metadata": {},
   "source": [
    "# Deviation of development between groups of embryos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0eb452",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [General](#first-bullet)\n",
    "* [Load paths](#second-bullet)\n",
    "* [Load model](#third-bullet)\n",
    "* [Calculate embeddings](#fourth-bullet)\n",
    "* [(Optional) Save embeddings](#fifth-bullet)\n",
    "* [Calculate similarities](#sixth-bullet)\n",
    "* [(Optional) Save similarities](#sixth-bullet)\n",
    "* [Overview of plot parameters ](#seventh-bullet)\n",
    "* [Plot similarities of batch comparisons with batch of untreated embryos ](#eigth-bullet)\n",
    "* [Plot similarities of all comparisons as individual curves ](#ninth-bullet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e5eb03",
   "metadata": {},
   "source": [
    "## General <a class=\"anchor\" id=\"first-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5771994",
   "metadata": {},
   "source": [
    "General imports and class definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f774ee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe40e37d",
   "metadata": {},
   "source": [
    "Load paths from config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cded940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from twinnet_tools.tnconfig import ProjectConfig\n",
    "\n",
    "config = ProjectConfig(\"twinnet_config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607c7266",
   "metadata": {},
   "source": [
    "Import Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76360d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import json\n",
    "import math\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_io as tfio\n",
    "from tensorflow.keras import applications, layers, models\n",
    "from scipy.stats import mannwhitneyu\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "import matplotlib.pylab as pl\n",
    "\n",
    "from twinnet_tools.tngeneral import TNToolsGeneral\n",
    "from twinnet_tools.tnbatchcomparison import TNToolsBatchComparison\n",
    "from twinnet_tools.tninference import TNToolsEmbeddings\n",
    "from twinnet_tools.tninference import TNToolsSimilarities\n",
    "from twinnet_tools.tnmodel import TNToolsNetwork\n",
    "from twinnet_tools.tnplot import TNToolsPlot\n",
    "from twinnet_tools.tnplotcompare import TNToolsPlotCompare\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b6684c",
   "metadata": {},
   "source": [
    "Prepare class instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3533b5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_general = TNToolsGeneral()\n",
    "tools_embeddings = TNToolsEmbeddings(size_img=224, size_img_min=250)\n",
    "tools_model = TNToolsNetwork()\n",
    "tools_similarities = TNToolsSimilarities()\n",
    "tools_plot = TNToolsPlot()\n",
    "tools_plot_compare = TNToolsPlotCompare()\n",
    "tools_batchcomparison = TNToolsBatchComparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e36fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_image_tiff_parse(path_img, img_size=224, img_size_min=250):\n",
    "    \"\"\"Load TIFF image from path.\"\"\"\n",
    "    img1 = tf.io.read_file(path_img)\n",
    "    img2 = tfio.experimental.image.decode_tiff(img1)\n",
    "    img3 = tf.image.resize_with_crop_or_pad(img2, img_size_min, img_size_min)\n",
    "    img4 = tf.reshape(img3, (img_size_min, img_size_min, 4))\n",
    "    img5 = tf.image.resize(img4, (img_size, img_size))\n",
    "    img6 = tfio.experimental.color.rgba_to_rgb(img5)\n",
    "    img7 = tf.keras.applications.resnet50.preprocess_input(img6)\n",
    "    return img7\n",
    "\n",
    "def fn_images_tiff_parse(paths_images, **kwargs):\n",
    "    \"\"\"Load multiple tiff images from paths to numpy array with tfio.\"\"\"\n",
    "    image_segments = list()\n",
    "    num_images = len(paths_images)\n",
    "\n",
    "    for i in range(num_images):\n",
    "        print(f'[LOADING] Image arrays {i + 1}/{num_images} ...'.ljust(50), end='\\r')\n",
    "        path_image = paths_images[i]\n",
    "        try:\n",
    "            image_segment = fn_image_tiff_parse(path_image, **kwargs)\n",
    "            \n",
    "            image_segments.append(image_segment)\n",
    "        except cv2.error:\n",
    "            pass\n",
    "    return np.array(image_segments)\n",
    "\n",
    "def list_to_embeddings(list_embryos_images, model_embedding):\n",
    "    \"\"\"Generate embeddings for an image set of embryos.\"\"\"\n",
    "    array_imgs = fn_images_tiff_parse(list_embryos_images)\n",
    "    embeds_imgs = tools_embeddings.imgs_to_embeddings(model_embedding, array_imgs)\n",
    "    return embeds_imgs\n",
    "\n",
    "\n",
    "def get_all_embeddings(dir_segments, tn_model):\n",
    "\n",
    "    # Get the list of folder names and sort them\n",
    "    sorted_folder_names = sorted(os.listdir(dir_segments))\n",
    "\n",
    "    # Get  embeddings\n",
    "    embeddings_reference = []\n",
    "    for folder_name in sorted_folder_names:\n",
    "        subfolder_path = os.path.join(dir_segments, folder_name)\n",
    "        # Check if the current path is a directory\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            print(\"Processing subfolder:\", folder_name)\n",
    "            # Load test image paths\n",
    "            imgs_src = sorted(glob.glob(f'{subfolder_path}/*.tif'))\n",
    "            # Calculate embedings\n",
    "            embeddings_reference.append(list_to_embeddings(imgs_src, tn_model))\n",
    "            \n",
    "    return embeddings_reference\n",
    "\n",
    "def cosine_similarity(A, B):\n",
    "    # Calculate the dot product between each pair of rows\n",
    "    dot_product = np.sum(A * B, axis=1)\n",
    "    \n",
    "    # Calculate the norms of each row using np.linalg.norm\n",
    "    norm_A = np.linalg.norm(A, axis=1)\n",
    "    norm_B = np.linalg.norm(B, axis=1)\n",
    "    \n",
    "    # Calculate the cosine similarity\n",
    "    similarity = dot_product / (norm_A * norm_B)\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "def get_similarities_to_reference(embeddings_reference, embeddings_test):\n",
    "\n",
    "    Number_anchors = len(embeddings_reference)\n",
    "    similarities = [] \n",
    "    for k in range(0, Number_anchors):\n",
    "        if np.array_equal(embeddings_reference[k],embeddings_test) == False: \n",
    "            similarities.append(cosine_similarity(embeddings_reference[k], embeddings_test))    \n",
    "    \n",
    "    similarities = np.vstack(similarities)\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "# Estimates the median distance per embryo and the the average of all distances \n",
    "def get_median_similarities_to_reference(embeddings_reference, embeddings_evaluation):\n",
    "\n",
    "    N_embryos = len(embeddings_evaluation)\n",
    "    median_similarities = [None] * N_embryos\n",
    "\n",
    "    for counter in range(0,N_embryos):\n",
    "        similarities_eval = get_similarities_to_reference(embeddings_reference, embeddings_evaluation[counter])\n",
    "        median_similarities[counter] = np.median(similarities_eval, axis=0)\n",
    "\n",
    "    median_similarities = np.vstack(median_similarities) \n",
    "\n",
    "\n",
    "    return median_similarities\n",
    "\n",
    "\n",
    "# Takes all combinations\n",
    "def get_ensamble_similarities_to_reference(embeddings_reference, embeddings_evaluation):\n",
    "\n",
    "    N_embryos = len(embeddings_evaluation)\n",
    "    similarities = [None] * N_embryos\n",
    "\n",
    "    for counter in range(0,N_embryos):\n",
    "        similarities_eval = get_similarities_to_reference(embeddings_reference, embeddings_evaluation[counter])\n",
    "        similarities[counter] = similarities_eval\n",
    "\n",
    "    similarities = np.concatenate(similarities, axis=0)\n",
    "\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "\n",
    "# function to get ramdom samples\n",
    "def generate_unique_integers(N, M):\n",
    "    # genraes N ramdom numbers between 0 and M\n",
    "    if N > M + 1:\n",
    "        raise ValueError(\"N must be less than or equal to M + 1 to ensure unique integers.\")\n",
    "    \n",
    "    unique_integers = np.random.choice(np.arange(M+1), N, replace=False)\n",
    "    return unique_integers\n",
    "\n",
    "def randomly_choose_elements(original_list, N):\n",
    "    indices = generate_unique_integers(N, len(original_list)-1)\n",
    "    new_list = [original_list[i] for i in indices]\n",
    "    return new_list, indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_statistics(ground_truth, predicted_classes):\n",
    "    # Accuracy: Fraction of correctly classified samples\n",
    "    accuracy = accuracy_score(ground_truth, predicted_classes)\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1db66a",
   "metadata": {},
   "source": [
    "Adjust matplotlib parameters to save plots as .svg files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd88879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rc_params = {'text.usetex': False,\n",
    "                'svg.fonttype': 'none'}\n",
    "mpl.rcParams.update(new_rc_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9672d0b4",
   "metadata": {},
   "source": [
    "## Load paths <a class=\"anchor\" id=\"second-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1479e398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Data : DataSet to analyze and reference information\n",
    "# Information for trained example models and download links\n",
    "# can be found in the Twin Network Github repository\n",
    "# (https://github.com/mueller-lab/TwinNet/tree/main/data)\n",
    "\n",
    "# The model used in this script is the trained example model\n",
    "# 'model_trained_more_data' for zebrafish embryos. Change the\n",
    "# Change the name to fit the Twin Network model version to be used\n",
    "modelName = 'ModelTN_v2'\n",
    "data_to_analyze = 'segments_EN_Acquifer'\n",
    "data_path = ' '\n",
    "\n",
    "ref_name = 'NORMAL'\n",
    "eval_names = ['BMP', 'NODAL', 'FGF','PCP', 'RA', 'SSH', 'WNT']\n",
    "\n",
    "# For calssification\n",
    "N_samples = 20  # number of random samples\n",
    "alpha = 0.01    # max p-value\n",
    "min_frac_diff = 0.3 # minimum fraction of frame deviated from normal\n",
    "max_N_embryos = 44\n",
    "n_rep = 5 # number of repetitions\n",
    "\n",
    "\n",
    "# Set input paths\n",
    "srcdir                 = data_path + data_to_analyze\n",
    "dir_segments_ref       = srcdir + '/segments_'+ref_name+'/'\n",
    "\n",
    "\n",
    "\n",
    "## Output paths\n",
    "outName = data_to_analyze + '_' + modelName\n",
    "dir_dst_embeddings =  Path(data_path + '/embeddings_new/embeddings_' + outName + '/')\n",
    "\n",
    "outpath       = data_path + 'Results/'\n",
    "outpathDetail = Path(outpath + 'AllGraphs/' + outName + '/')\n",
    "outNameSufix  = '_' + outName + '.svg'\n",
    "outFileNameV1 = outpath + 'results' + outNameSufix\n",
    "\n",
    "\n",
    "srcVideopath = srcdir+'/Representative/aligned/'\n",
    "\n",
    "np.random.seed(1) # for reproducibility \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de54c39",
   "metadata": {},
   "source": [
    "Prepare directory to save output files to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050d25f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "outpathDetail.mkdir(parents=True, exist_ok=True)\n",
    "dir_dst_embeddings.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "## Print analysis description\n",
    "modelpath = os.path.join(data_path, modelName+'.h5')\n",
    "\n",
    "print('Analizing ' + data_to_analyze + 'with model: ' + modelName)\n",
    "print(data_to_analyze + ': ' + srcdir)\n",
    "print(modelName + ' : ' + modelpath)\n",
    "print('Saving detail results at : ' +str(outpathDetail))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829679b6",
   "metadata": {},
   "source": [
    "## Load model <a class=\"anchor\" id=\"third-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295eed11",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_model_embedding = tools_model.tn_embedding_load(modelpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5f9e69",
   "metadata": {},
   "source": [
    "## Calculate embeddings <a class=\"anchor\" id=\"fourth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2467dfd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get reference embeddings\n",
    "All_embeddings_reference  = get_all_embeddings(dir_segments_ref,  tn_model_embedding)\n",
    "\n",
    "All_embeddings_evaluation = []\n",
    "\n",
    "for eval_name in eval_names:\n",
    "    dir_segments_eval      = srcdir + '/segments_'+eval_name+'/'\n",
    "    All_embeddings_evaluation.append(get_all_embeddings(dir_segments_eval, tn_model_embedding))\n",
    "    print(f\"Analyzing : {len(All_embeddings_evaluation)} embryos against {len(All_embeddings_reference)} reference ones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de63b3d9",
   "metadata": {},
   "source": [
    "## Calculate similarities <a class=\"anchor\" id=\"sixth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a04cb5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "All_accuracy = []\n",
    "for All_embeddings_eval, name in zip(All_embeddings_evaluation, eval_names):\n",
    "\n",
    "    if max_N_embryos > len(All_embeddings_eval):\n",
    "        max_N_embryos_i = len(All_embeddings_eval)\n",
    "    else:\n",
    "        max_N_embryos_i = max_N_embryos\n",
    "        \n",
    "\n",
    "    accuracy = np.full((n_rep, max_N_embryos), np.nan)\n",
    "\n",
    "    for i  in tqdm(range(n_rep), desc='Processing '+name):\n",
    "        for N_embryos in np.arange(3, max_N_embryos_i+1, 1):\n",
    "\n",
    "            ground_truth = np.ones((N_samples,), dtype=int)  # 1 abnormal, for all\n",
    "            predicted_classes = np.zeros((N_samples,), dtype=int)  # 0 normal, 1 abnormal\n",
    "\n",
    "            for itt in range(N_samples):\n",
    "                # get a ramdon sample of N_embryos\n",
    "                embeddings_reference, indexes_ref = randomly_choose_elements(All_embeddings_reference, N_embryos)\n",
    "                embeddings_evaluation , indexes_eval = randomly_choose_elements(All_embeddings_eval, N_embryos)\n",
    "\n",
    "                # get all similarities\n",
    "                similarities     = get_median_similarities_to_reference(embeddings_reference, embeddings_evaluation)\n",
    "                similarities_ref = get_median_similarities_to_reference(embeddings_reference, embeddings_reference)\n",
    "\n",
    "                # Perform the Mann-Whitney U test\n",
    "                statistic, p_value = mannwhitneyu(similarities, similarities_ref, alternative='less', axis=0)   \n",
    "\n",
    "                # get percentage of embryo frames different from the control\n",
    "                frac_diff = sum(1 for x in p_value if x < alpha)\n",
    "                frac_diff = frac_diff / len(p_value)\n",
    "\n",
    "                # classify sample_eval\n",
    "                if frac_diff > min_frac_diff:\n",
    "                    predicted_classes[itt] = 1\n",
    "\n",
    "            # get stats fo the classification\n",
    "            #accuracy[i, N_embryos-1], sensitivity[i, N_embryos-1], precision[i, N_embryos-1], f_score[i, N_embryos-1] = calculate_statistics(ground_truth, predicted_classes)\n",
    "            accuracy[i, N_embryos-1] = calculate_statistics(ground_truth, predicted_classes)\n",
    "\n",
    "            #print(f\"Analyzing: {N_samples} samples with {N_embryos} embryos, accuracy:{accuracy[i, N_embryos]:.3f}, sensitivity:{sensitivity[i, N_embryos]:.3f}, precision:{precision[i, N_embryos]:.3f}, f_score:{f_score[i, N_embryos]:.3f}\")\n",
    "\n",
    "    # Store data\n",
    "    All_accuracy.append(accuracy)\n",
    "                       \n",
    "    # Plotting \n",
    "    N_values = np.arange(1, max_N_embryos_i+1, 1)[2:max_N_embryos_i]\n",
    "    accuracies = np.nanmean(accuracy[:,2:max_N_embryos_i], axis=0)\n",
    "    std_accuracies = np.nanstd(accuracy[:,2:max_N_embryos_i], axis=0)\n",
    "    # Plotting the metrics\n",
    "    fig, axs = plt.subplots(dpi=300, figsize=(4,3))\n",
    "    plt.plot(N_values, accuracies, label='Acuracy '+ name)\n",
    "    plt.fill_between(N_values,accuracies-std_accuracies, accuracies+std_accuracies,  alpha=0.2)\n",
    "\n",
    "    plt.xlabel('N_embryos')\n",
    "    plt.ylabel('Metric Value')\n",
    "    plt.title('Metrics vs N_embryos')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1b6409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all accuracy\n",
    "name2plot  = ['BMP', 'PCP', 'FGF', 'SSH',  'NODAL',  'RA',  'WNT']\n",
    "indexesT = [eval_names.index(name) for name in name2plot]\n",
    "\n",
    "print(indexesT)\n",
    "\n",
    "#colors = pl.cm.tab20(np.linspace(0,1,len(indexesT)))\n",
    "colors = pl.cm.viridis(np.linspace(0,1,len(indexesT)+1))\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(dpi=300, figsize=(5,3))\n",
    "N_values = np.arange(1, max_N_embryos+1, 1)\n",
    "for i, (idx) in enumerate(indexesT, 0):\n",
    "    accuracy = All_accuracy[idx]\n",
    "    name = eval_names[idx]\n",
    "    \n",
    "    print(name)\n",
    "#for i, (accuracy, name) in enumerate(zip(All_accuracy, eval_names), 0):\n",
    "\n",
    "    accuracies = np.nanmean(accuracy, axis=0)\n",
    "    std_accuracies = np.nanstd(accuracy, axis=0)\n",
    "    plt.plot(N_values, accuracies, label=name, color=colors[i+1])\n",
    "    plt.fill_between(N_values,accuracies-std_accuracies, accuracies+std_accuracies, facecolor=colors[i+1], alpha=0.2)\n",
    "\n",
    "    \n",
    "plt.xlim(0, max_N_embryos)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(fontsize='xx-small')\n",
    "plt.xlabel('# embryos per sample',  fontsize=10)\n",
    "plt.ylabel('Detection accuracy', fontsize=10)\n",
    "#plt.xticks(np.arange(0, max_N_embryos, 4), fontsize=6)          \n",
    "#plt.yticks(np.arange(0, 1, 10), fontsize=6)\n",
    "\n",
    "outname =  os.path.join(outpath, data_to_analyze+'_accuracy.svg')\n",
    "plt.savefig(outname, bbox_inches='tight', dpi=300)      \n",
    "#plt.show()  \n",
    "plt.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820ab3fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Plot some examples\n",
    "# Calculate similarities for N randomly chosen embryos\n",
    "np.random.seed(1) # for reproducibility \n",
    "embeddings_reference, index_ref = randomly_choose_elements(All_embeddings_reference,max_N_embryos)\n",
    "\n",
    "\n",
    "for i, (idx) in enumerate(indexesT, 0):\n",
    "    All_embeddings_eval = All_embeddings_evaluation[idx]\n",
    "    name = eval_names[idx]\n",
    "    \n",
    "#for i, (All_embeddings_eval, name) in enumerate(zip(All_embeddings_evaluation, eval_names),0):\n",
    "\n",
    "    if max_N_embryos > len(All_embeddings_eval):\n",
    "        N_embryos = len(All_embeddings_eval)\n",
    "    else:\n",
    "        N_embryos = max_N_embryos\n",
    "            \n",
    "    embeddings_evaluation, index_eval = randomly_choose_elements(All_embeddings_eval, N_embryos)\n",
    "\n",
    "    print(f\"Analyzing {name}: {len(embeddings_evaluation)} embryos against {len(embeddings_reference)} reference ones\")\n",
    "    print(f\"    from  : {len(All_embeddings_eval)} embryos against {len(All_embeddings_reference)} reference ones\")\n",
    "\n",
    "\n",
    "    similarities     = get_median_similarities_to_reference(embeddings_reference, embeddings_evaluation)\n",
    "    similarities_ref = get_median_similarities_to_reference(embeddings_reference, embeddings_reference)\n",
    "\n",
    "    average = np.mean(similarities, axis=0)\n",
    "    std_dev = np.std(similarities, axis=0)\n",
    "    average_ref = np.mean(similarities_ref, axis=0)\n",
    "    std_dev_ref = np.std(similarities_ref, axis=0)\n",
    "    \n",
    "    vals_up = average - std_dev\n",
    "    vals_down = average + std_dev\n",
    "    vals_up_ref = average_ref - std_dev_ref\n",
    "    vals_down_ref = average_ref + std_dev_ref\n",
    "\n",
    "    \n",
    "    # Perform the Mann-Whitney U test\n",
    "    statistic, p_value = mannwhitneyu(similarities, similarities_ref, alternative='less', axis=0)\n",
    "                       \n",
    "    frac_diff = sum(1 for x in p_value if x < alpha)\n",
    "    frac_diff = 100 * frac_diff / len(p_value)\n",
    "\n",
    "    print(f\"    frac_diff: {frac_diff:.3f} for {name}\")\n",
    "\n",
    "    x = np.array(range(0,similarities.shape[1]))*2.0\n",
    "    \n",
    "    fig, axs = plt.subplots(dpi=300, figsize=(4,2))\n",
    "    \n",
    "    plt.scatter(x, average, label=f\"{name}: {frac_diff:.2f} % diff\", color=colors[i+1], marker='.', s=1)\n",
    "    plt.fill_between(x, vals_up, vals_down, alpha=0.2, facecolor=colors[i+1])\n",
    "    plt.ylim(0.5, 1.0)\n",
    "    plt.xlim(0, 1440)\n",
    "    plt.xticks(np.arange(0, 1600, 200), fontsize=6)\n",
    "    plt.yticks(np.arange(0.5, 1.01, 0.1), fontsize=6)\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time (min)',  fontsize=8)\n",
    "    plt.ylabel('Cosine similarity ϕ', fontsize=8)\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    \n",
    "    outname =  os.path.join(outpath, data_to_analyze+'_'+name+'_similarity.svg')\n",
    "    plt.savefig(outname, bbox_inches='tight', dpi=300)      \n",
    "    #    plt.show()  \n",
    "    plt.close() \n",
    "    \n",
    "    fig, axs = plt.subplots(dpi=300, figsize=(4,1))\n",
    "    plt.scatter(x, np.log10(p_value), color=colors[i+1], marker='.', s=1)\n",
    "    plt.axhline(y=np.log10(alpha), color='r', linestyle='--', label=f\"p-value: {alpha:.3f}\")\n",
    "    plt.xlabel('Time (min)',  fontsize=8)\n",
    "    plt.ylabel('ln(p-value)', fontsize=8) \n",
    "    plt.ylim(-20, 0)\n",
    "    plt.xlim(0, 1440)\n",
    "    plt.xticks(np.arange(0, 1600, 200), fontsize=6)\n",
    "    plt.yticks(np.arange(-20, 0, 5), fontsize=6)\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "    outname =  os.path.join(outpath, data_to_analyze+'_'+name+'_p_values.svg')\n",
    "    plt.savefig(outname, bbox_inches='tight', dpi=300)      \n",
    "    #    plt.show()  \n",
    "    plt.close() \n",
    "    \n",
    "    # save plo refrence\n",
    "    if i == 0:\n",
    "        fig, axs = plt.subplots(dpi=300, figsize=(4,2))\n",
    "        plt.scatter(x, average_ref, label='Untreated', color=\"#c2df23\", marker='.', s=1)\n",
    "        plt.fill_between(x, vals_up_ref, vals_down_ref, alpha=0.2, color=\"#c2df23\")\n",
    "        plt.ylim(0.5, 1.0)\n",
    "        plt.xlim(0, 1440)\n",
    "        plt.xticks(np.arange(0, 1600, 200), fontsize=6)\n",
    "        plt.yticks(np.arange(0.5, 1.01, 0.1), fontsize=6)\n",
    "        plt.legend()\n",
    "        plt.xlabel('Time (min)',  fontsize=8)\n",
    "        plt.ylabel('Cosine similarity', fontsize=8)\n",
    "        plt.gca().spines['top'].set_visible(False)\n",
    "        plt.gca().spines['right'].set_visible(False)\n",
    "        outname =  os.path.join(outpath, data_to_analyze+'_Untreated_similarity.svg')\n",
    "        plt.savefig(outname, bbox_inches='tight', dpi=300)      \n",
    "        #    plt.show()  \n",
    "        plt.close()        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b86f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "eval_names_Title = ['-BMP', '-NODAL', '-FGF','-PCP', '+RA', '-SSH', '-WNT']\n",
    "\n",
    "# Calculate similarities for N randomly chosen embryos\n",
    "np.random.seed(1) # for reproducibility \n",
    "\n",
    "# For each phenotype\n",
    "for i, (idx) in enumerate(indexesT, 0):\n",
    "\n",
    "\n",
    "    All_embeddings_eval = All_embeddings_evaluation[idx]\n",
    "    nameId = eval_names[idx]\n",
    "    titleId = eval_names_Title[idx]\n",
    "    \n",
    "    if nameId == 'SSH':\n",
    "        nameId = 'SHH'\n",
    "\n",
    "    if max_N_embryos > len(All_embeddings_eval):\n",
    "            N_embryos = len(All_embeddings_eval)\n",
    "    else:\n",
    "        N_embryos = max_N_embryos\n",
    "\n",
    "    embeddings_evaluation, indexes = randomly_choose_elements(All_embeddings_eval, N_embryos)\n",
    "\n",
    "    print(f\"Analyzing {nameId}: {len(embeddings_evaluation)} embryos against {len(embeddings_reference)} reference ones\")\n",
    "    print(f\"    from  : {len(All_embeddings_eval)} embryos against {len(All_embeddings_reference)} reference ones\")\n",
    "\n",
    "    similarities     = get_median_similarities_to_reference(embeddings_reference, embeddings_evaluation)\n",
    "\n",
    "    average = np.mean(similarities, axis=0)\n",
    "    std_dev = np.std(similarities, axis=0)    \n",
    "    vals_up = average - std_dev\n",
    "    vals_down = average + std_dev\n",
    "\n",
    "\n",
    "    folder_path = srcVideopath+nameId\n",
    "    folder_path_ref = srcVideopath+'NORMAL/'\n",
    "    out_name_video = outpath+nameId+'_aligned.mp4'\n",
    "\n",
    "    from matplotlib.animation import FuncAnimation\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, dpi=300, figsize=(12, 4))\n",
    "\n",
    "    # Get a list of image files in the folder\n",
    "    tiff_images_list = sorted([os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if filename.lower().endswith((\".tiff\", \".tif\"))])\n",
    "    tiff_images_list_ref = sorted([os.path.join(folder_path_ref, filename) for filename in os.listdir(folder_path_ref) if filename.lower().endswith((\".tiff\", \".tif\"))])\n",
    "\n",
    "    def update(frame):\n",
    "\n",
    "        axs[0].clear()\n",
    "        axs[0].scatter(x, average, color=colors[i+1], marker='.', s=1)\n",
    "        axs[0].fill_between(x, vals_up, vals_down, alpha=0.2, facecolor=colors[i+1])\n",
    "        axs[0].set_ylim(0.5, 1.0)\n",
    "        axs[0].set_xlim(0, 1440)\n",
    "        axs[0].set_xticks(np.arange(0, 1600, 200))\n",
    "        axs[0].set_yticks(np.arange(0.5, 1.01, 0.1))\n",
    "        axs[0].set_xlabel('Time (min)', fontsize=8)\n",
    "        axs[0].set_ylabel('Cosine similarity ϕ', fontsize=8)\n",
    "        axs[0].spines['top'].set_visible(False)\n",
    "        axs[0].spines['right'].set_visible(False)\n",
    "        axs[0].axvline(x=2*frame, color='red', linestyle='--', label='Perpendicular Line')\n",
    "        #axs[0].legend()\n",
    "\n",
    "        # Load and display different images for each frame\n",
    "        image = cv2.imread(tiff_images_list[frame])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image_ref = cv2.imread(tiff_images_list_ref[frame])\n",
    "        image_ref = cv2.cvtColor(image_ref, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        axs[1].clear()\n",
    "        axs[1].imshow(image_ref)\n",
    "        axs[1].set_title('Untreated', fontsize=10)\n",
    "\n",
    "        axs[2].clear()\n",
    "        axs[2].imshow(image)\n",
    "        axs[2].set_title(nameId, fontsize=10)\n",
    "        axs[2].set_title(titleId, fontsize=10, style='italic')\n",
    "\n",
    "\n",
    "        # Remove ticks from the image subplots\n",
    "        axs[1].set_xticks([])\n",
    "        axs[1].set_yticks([])\n",
    "        axs[2].set_xticks([])\n",
    "        axs[2].set_yticks([])\n",
    "\n",
    "        # Adjust the layout to fill the upper panel\n",
    "        plt.subplots_adjust(top=0.8, bottom=0.2)\n",
    "\n",
    "\n",
    "    animation = FuncAnimation(fig, update, frames=len(tiff_images_list), interval=50)\n",
    "\n",
    "    # Save the animation as a video\n",
    "    animation.save(out_name_video, writer='ffmpeg', dpi=300)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6225be4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get representative example\n",
    "np.random.seed(1) # for reproducibility \n",
    "for i, (idx) in enumerate(indexesT, 0):\n",
    "    All_embeddings_eval = All_embeddings_evaluation[idx]\n",
    "    name = eval_names[idx]\n",
    "    \n",
    "#for i, (All_embeddings_eval, name) in enumerate(zip(All_embeddings_evaluation, eval_names),0):\n",
    "\n",
    "    if max_N_embryos > len(All_embeddings_eval):\n",
    "        N_embryos = len(All_embeddings_eval)\n",
    "    else:\n",
    "        N_embryos = max_N_embryos\n",
    "            \n",
    "    embeddings_evaluation, indexes = randomly_choose_elements(All_embeddings_eval, N_embryos)\n",
    "\n",
    "    print(f\"Analyzing {name}: {len(embeddings_evaluation)} embryos against {len(embeddings_reference)} reference ones\")\n",
    "    print(f\"    from  : {len(All_embeddings_eval)} embryos against {len(All_embeddings_reference)} reference ones\")\n",
    "\n",
    "    similarities     = get_median_similarities_to_reference(embeddings_reference, embeddings_evaluation)\n",
    "    \n",
    "\n",
    "    average = np.mean(similarities, axis=0)\n",
    "    std_dev = np.std(similarities, axis=0)    \n",
    "    vals_up = average - std_dev\n",
    "    vals_down = average + std_dev\n",
    "\n",
    "    errors = np.linalg.norm(similarities - average, axis=1)\n",
    "    closest_row_index = np.argmin(errors)\n",
    "    \n",
    "    dir_segments_eval      = srcdir + '/segments_'+name+'/'\n",
    "    subfolders = sorted(item for item in os.listdir(dir_segments_eval) if os.path.isdir(os.path.join(dir_segments_eval, item)))\n",
    "\n",
    "    print(f\"    Closest embryo to average: {subfolders[indexes[closest_row_index]]}\")\n",
    "\n",
    "    \n",
    "    # Perform the Mann-Whitney U test\n",
    "    statistic, p_value = mannwhitneyu(similarities, similarities_ref, alternative='less', axis=0)\n",
    "                       \n",
    "    frac_diff = sum(1 for x in p_value if x < alpha)\n",
    "    frac_diff = 100 * frac_diff / len(p_value)\n",
    "\n",
    "    print(f\"    frac_diff: {frac_diff:.3f} for {name}\")\n",
    "\n",
    "    x = np.array(range(0,similarities.shape[1]))*2.0\n",
    "    \n",
    "    fig, axs = plt.subplots(dpi=300, figsize=(4,2))\n",
    "    \n",
    "    plt.scatter(x, average, label=f\"{name}: {frac_diff:.2f} % diff\", color=colors[i+1], marker='.', s=1)\n",
    "    plt.fill_between(x, vals_up, vals_down, alpha=0.2, facecolor=colors[i+1])\n",
    "    plt.plot(x, similarities[closest_row_index], lw=0.5, color='black')\n",
    "    plt.ylim(0.5, 1.0)\n",
    "    plt.xlim(0, 1440)\n",
    "    plt.xticks(np.arange(0, 1600, 200), fontsize=6)\n",
    "    plt.yticks(np.arange(0.5, 1.01, 0.1), fontsize=6)\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time (min)',  fontsize=8)\n",
    "    plt.ylabel('Cosine similarity', fontsize=8)\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    \n",
    "    outname =  os.path.join(outpath, data_to_analyze+'_'+name+'_similarity.svg')\n",
    "    #plt.savefig(outname, bbox_inches='tight', dpi=300)      \n",
    "    plt.show()  \n",
    "    #plt.close() \n",
    "    \n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0172a97a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
